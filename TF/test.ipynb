{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = pipeline(\"text2text-generation\",model=\"t5-small-FT-false_statements\\checkpoint-11700\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example: The former Chief of General Staff Franz Halder was instrumental in spreading this legend in war historiography. Together with other officers in the United States Army's war history research group, the Operational History (German) Section, he produced studies on warfare that characterized the Wehrmacht's warfare as decent and heroic. \n",
      "false_statement: Franz Halder was not instrumental in spreading this legend in war historiography.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\newzn\\Projects\\AutoQuiz\\.venv\\Lib\\site-packages\\transformers\\generation\\utils.py:1258: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "example = \"The former Chief of General Staff Franz Halder was instrumental in spreading this legend in war historiography. Together with other officers in the United States Army's war history research group, the Operational History (German) Section, he produced studies on warfare that characterized the Wehrmacht's warfare as decent and heroic. \"#pd.read_csv(\"false_statements.csv\")['text'][3]\n",
    "false_statement = pipe(example)[0]['generated_text']\n",
    "print(f\"Example: {example}\")\n",
    "print(f\"false_statement: {false_statement}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "model.safetensors:   0%|          | 0.00/242M [00:00<?, ?B/s]\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "model.safetensors:   0%|          | 197k/242M [00:00<02:11, 1.84MB/s]\n",
      "\n",
      "scheduler.pt: 100%|██████████| 1.06k/1.06k [00:00<00:00, 2.00kB/s]/s]\n",
      "rng_state.pth: 100%|██████████| 14.2k/14.2k [00:00<00:00, 25.9kB/s]/s]\n",
      "spiece.model: 100%|██████████| 792k/792k [00:00<00:00, 983kB/s] 6MB/s]\n",
      "model.safetensors:   4%|▍         | 10.3M/242M [00:00<00:16, 14.4MB/s]\n",
      "training_args.bin: 100%|██████████| 5.37k/5.37k [00:00<00:00, 32.2kB/s]\n",
      "model.safetensors: 100%|██████████| 242M/242M [00:17<00:00, 14.2MB/s] \n",
      "\n",
      "\n",
      "\n",
      "optimizer.pt: 100%|██████████| 484M/484M [00:25<00:00, 19.1MB/s]\n",
      "\n",
      "\n",
      "\n",
      "Upload 6 LFS files: 100%|██████████| 6/6 [00:25<00:00,  4.31s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/blago7daren/TF_false_QG/commit/65c73a5abb32b208f157faceab5c983550b7a863', commit_message='Upload folder using huggingface_hub', commit_description='', oid='65c73a5abb32b208f157faceab5c983550b7a863', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from huggingface_hub import HfApi, create_repo\n",
    "\n",
    "api = HfApi()\n",
    "\n",
    "# Create the repository\n",
    "create_repo(repo_id=\"blago7daren/TF_false_QG\", repo_type=\"model\")\n",
    "\n",
    "# Then upload the folder\n",
    "api.upload_folder(\n",
    "    folder_path=r\"C:\\Users\\newzn\\Projects\\AutoQuiz\\TF\\models\\TF_false_QG\",\n",
    "    repo_id=\"blago7daren/TF_false_QG\",\n",
    "    repo_type=\"model\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import HfApi, create_repo\n",
    "\n",
    "api = HfApi()\n",
    "\n",
    "# Create the repository\n",
    "create_repo(repo_id=\"blago7daren/TF_false_QG\", repo_type=\"model\")\n",
    "\n",
    "# Then upload the folder\n",
    "api.upload_folder(\n",
    "    folder_path=r\"C:\\Users\\newzn\\Projects\\AutoQuiz\\TF\\models\\TF_true_QG\",\n",
    "    repo_id=\"blago7daren/TF_false_QG\",\n",
    "    repo_type=\"model\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
